{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26944871",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import re, time, json,os,csv\n",
    "from app.utils import Helper\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from app.constants import *\n",
    "from collections import defaultdict\n",
    "\n",
    "path = INPUT_PATH\n",
    "def sorted_rate_files(folder_path):\n",
    "    files = os.listdir(folder_path)\n",
    "    rate_files = [f for f in files if f.startswith(\"RealTime_\")]\n",
    "    rate_files.sort(key=lambda x: int(x.split(\"_\")[1].split(\".\")[0]))\n",
    "    return rate_files\n",
    " \n",
    "# Ticker\tDate\tTime\tLTP\tBuyPrice\tBuyQty\tSellPrice\tSellQty\tLTQ\tOpenInterest\n",
    "# RIIL.NSE\t19-05-2025\t09:15:03\t928.6\t0\t0\t0\t0\t0\t0\n",
    "# RIIL.NSE\t19-05-2025\t09:15:03\t929.7\t926.2\t14\t929.7\t7\t1001\t0\n",
    "# RIIL.NSE\t19-05-2025\t09:15:03\t927.95\t926.2\t14\t929.7\t7\t0\t0\n",
    "# RIIL.NSE\t19-05-2025\t09:15:15\t933.65\t930.3\t10\t933.65\t75\t2079\t0\n",
    "# RIIL.NSE\t19-05-2025\t09:15:15\t933.65\t930.3\t10\t933.65\t75\t0\t0\n",
    "# RIIL.NSE\t19-05-2025\t09:15:25\t934\t931.45\t10\t934.45\t5\t322\t0\n",
    "# RIIL.NSE\t19-05-2025\t09:15:25\t934\t931.45\t10\t934.45\t5\t0\t0\n",
    "# RIIL.NSE\t19-05-2025\t09:15:34\t931.35\t932.2\t10\t933.95\t1\t1\t0\n",
    "# RIIL.NSE\t19-05-2025\t09:15:42\t932.4\t931.75\t10\t933.4\t3\t33\t0\n",
    "# RIIL.NSE\t19-05-2025\t09:15:51\t931.95\t931.65\t14\t933.5\t5\t275\t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b58648bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"exchange_mapping.json\"\n",
    "import json\n",
    "data = Helper.load_json(path)\n",
    "\n",
    "def_dict = {}\n",
    "\n",
    "for element in data:\n",
    "    def_dict.update({element[\"cogencis_symbol\"]:element[\"exchange_symbol\"]})\n",
    "    \n",
    "\n",
    "with open(\"fallback.json\",\"w\") as f:\n",
    "    json.dump(def_dict,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed735826",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TimeoutError: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\kaustubh.keny\\Projects\\office-work\\stock-ticker-feed\\app\\ftp_connector.py\", line 28, in ftp_file_transfer\n",
      "    with FTP(host) as ftp:\n",
      "         ~~~^^^^^^\n",
      "  File \"c:\\Users\\kaustubh.keny\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\ftplib.py\", line 121, in __init__\n",
      "    self.connect(host)\n",
      "    ~~~~~~~~~~~~^^^^^^\n",
      "  File \"c:\\Users\\kaustubh.keny\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\ftplib.py\", line 158, in connect\n",
      "    self.sock = socket.create_connection((self.host, self.port), self.timeout,\n",
      "                ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "                                         source_address=self.source_address)\n",
      "                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kaustubh.keny\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\socket.py\", line 864, in create_connection\n",
      "    raise exceptions[0]\n",
      "  File \"c:\\Users\\kaustubh.keny\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\socket.py\", line 849, in create_connection\n",
      "    sock.connect(sa)\n",
      "    ~~~~~~~~~~~~^^^^\n",
      "TimeoutError: [WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from app.ftp_connector import ftp_file_transfer\n",
    "from app.logger import setup_logger, set_logger\n",
    "import logging\n",
    "# Initialize scheduler logger\n",
    "log_dir = r\"C:\\Users\\kaustubh.keny\\Projects\\OUTPUTS\\ticker-ops\\logs\"\n",
    "logger = setup_logger(name=\"scheduler\", log_dir=log_dir, console_level=logging.INFO)\n",
    "set_logger(logger)\n",
    "path = r\"C:\\Users\\kaustubh.keny\\Downloads\\lookup.csv\"\n",
    "\n",
    "ftp_file_transfer(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb8121d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NSF', 'NSFOA', 'NSOG'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = r\"C:\\Users\\kaustubh.keny\\Downloads\\SGDataOut17-9-2025.txt\"\n",
    "\n",
    "extensions = set()\n",
    "\n",
    "with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = f.readlines()\n",
    "    # for line in f:\n",
    "    #     print(line.strip())\n",
    "    for tick in data:\n",
    "        data = tick.split(\";\",1)[0]\n",
    "        data = data.split(\"CPR\")[-1]\n",
    "        data = data.split(\".\")[-1]\n",
    "        \n",
    "        extensions.add(data)\n",
    "        \n",
    "\n",
    "extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cd0a57e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Total Ticks: 3705688\n",
      "Malformed Lines in Source: 3699\n",
      "Valid Ticks Stored: 3701989\n"
     ]
    }
   ],
   "source": [
    "input_folder_path = r\"C:\\Users\\kaustubh.keny\\Projects\\INPUTS\\17-09-2025\"\n",
    "\n",
    "source_total_lines = 0\n",
    "source_malformed_lines = []\n",
    "source_symbol_tick_dict = {}\n",
    "\n",
    "\n",
    "# indexes = [\"AXISBANK.NS\",\"BHARELEC.NS\",\"GODFPHIL.NS\",\"HDFCBANK.NS\",\"KOTAMAHI.NS\",\"LARSTOUB.NS\",\"REDI.NS\",\"RELINDUS.NS\",\"TATASTEE.NS\"]\n",
    "indexes = [\"ITC.NS\",\"BHARHEAV.NS\",\"MARUSUZU.NS\",\"WIPR.NS\",\"EICHMOTO.NS\",\"SHRIFINA.NS\",\"IDFCFIRS.NS\",\"HCLTECH.NS\",\"JSWSTEE.NS\"]\n",
    "selected_symbols = defaultdict(list, {k: [] for k in indexes})\n",
    "\n",
    "for file in os.listdir(input_folder_path):\n",
    "    file_path = os.path.join(input_folder_path, file)\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        source_total_lines += len(lines)\n",
    "\n",
    "        for line in lines:\n",
    "            parts = line.strip().split(\"||\")\n",
    "            if len(parts) < 6:\n",
    "                source_malformed_lines.append(line.strip())\n",
    "                continue\n",
    "\n",
    "            symbol = parts[4].strip()\n",
    "            if symbol in selected_symbols:\n",
    "                selected_symbols[symbol].append(line.strip())\n",
    "            \n",
    "            source_symbol_tick_dict[symbol] = source_symbol_tick_dict.get(symbol, 0) + 1\n",
    "\n",
    "print(f\"Source Total Ticks: {source_total_lines}\")\n",
    "print(f\"Malformed Lines in Source: {len(source_malformed_lines)}\")\n",
    "print(f\"Valid Ticks Stored: {sum(source_symbol_tick_dict.values())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "606a8e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Total Ticks: 3701989\n",
      "Output Files Processed: 17356\n",
      "Files with Errors: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "output_folder_path = r\"C:\\Users\\kaustubh.keny\\Projects\\OUTPUTS\\ticker-ops\\data\\17-09-2025\"\n",
    "\n",
    "output_total_ticks = 0\n",
    "output_symbol_tick_dict = {}\n",
    "output_errors = []\n",
    "\n",
    "for file in os.listdir(output_folder_path):\n",
    "    file_path = os.path.join(output_folder_path, file)\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        ticks = df.shape[0] #excludes the header \n",
    "        symbol = file.replace(\".csv\", \"\")\n",
    "        output_symbol_tick_dict[symbol] = ticks\n",
    "        output_total_ticks += ticks\n",
    "    except Exception as e:\n",
    "        output_errors.append((file, str(e)))\n",
    "\n",
    "print(f\"Output Total Ticks: {output_total_ticks}\")\n",
    "print(f\"Output Files Processed: {len(output_symbol_tick_dict)}\")\n",
    "print(f\"Files with Errors: {len(output_errors)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64542c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ticks per index\n",
    "import pandas as pd\n",
    "output_cleaned = {k.replace('.csv', ''): v for k, v in output_symbol_tick_dict.items()}\n",
    "\n",
    "all_symbols = set(source_symbol_tick_dict.keys()) | set(output_cleaned.keys())\n",
    "tick_report = []\n",
    "for symbol in sorted(all_symbols):\n",
    "    source_count = source_symbol_tick_dict.get(symbol, 0)\n",
    "    output_count = output_cleaned.get(symbol, 0)\n",
    "    missing = source_count - output_count\n",
    "    tick_report.append((symbol, source_count, output_count, missing))\n",
    "\n",
    "# Display report\n",
    "tick_report = sorted(tick_report, key = lambda x: x[1], reverse=True)\n",
    "print(f\"{'Symbol':25} | {'Source':>6} | {'Output':>6} | {'Missing':>7}\")\n",
    "print(\"-\" * 55)\n",
    "for symbol, src, out, miss in tick_report:\n",
    "    print(f\"{symbol:25} | {src:6} | {out:6} | {miss:7}\")\n",
    "\n",
    "\n",
    "# df = pd.DataFrame(tick_report,columns=[\"Symbol\",\"Source\",\"Output\",\"Missing\"])\n",
    "# df.to_excel(\"Symbol_Count.xlsx\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d66c4e",
   "metadata": {},
   "source": [
    "DATA VALIDATION SHEET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e7f50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "priority_keys = [\"2\", \"4\", \"6\", \"5\", \"7\", \"3\"]\n",
    "key_to_label = {\n",
    "    \"10\": \"DATETIME\",\n",
    "    \"2\": \"Ltp\",\n",
    "    \"4\": \"BuyPrice\",   # bid\n",
    "    \"6\": \"BuyQty\",\n",
    "    \"5\": \"SellPrice\",  # ask\n",
    "    \"7\": \"SellQty\",\n",
    "    \"3\": \"Ltq\"\n",
    "}\n",
    "\n",
    "for name, values in selected_symbols.items():\n",
    "    cleaned_rows = []\n",
    "\n",
    "    for tick in values:\n",
    "        tick_data = tick.split(\"||\")[-1]\n",
    "        kv_pairs = [kv for kv in tick_data.split(\"~\") if \"=\" in kv]\n",
    "        kv_dict = {kv.split(\"=\")[0]: kv.split(\"=\")[1] for kv in kv_pairs}\n",
    "        timestamp = kv_dict.get(\"10\", None)\n",
    "        if timestamp and \" \" in timestamp:\n",
    "            date_part, time_part = timestamp.split(\" \")\n",
    "        else:\n",
    "            date_part, time_part = \"Nil\", \"Nil\"\n",
    "        row = [date_part, time_part]\n",
    "        for key in priority_keys:\n",
    "            row.append(kv_dict.get(key, \"\"))\n",
    "\n",
    "        remaining_keys = [k for k in kv_dict.keys() if k not in priority_keys and k != \"10\"]\n",
    "        for key in remaining_keys:\n",
    "            row.append(kv_dict[key])\n",
    "\n",
    "        cleaned_rows.append(row)\n",
    "\n",
    "    \n",
    "    header = [\"DATE\", \"TIME\"]\n",
    "    header += [key_to_label.get(k, f\"KEY_{k}\") for k in priority_keys]\n",
    "\n",
    "   \n",
    "    max_cols = max(len(row) for row in cleaned_rows)\n",
    "    while len(header) < max_cols:\n",
    "        extra_idx = len(header) - len(priority_keys) - 2 + 1  # offset for DATE, TIME\n",
    "        header.append(f\"EXTRA_COL_{extra_idx}\")\n",
    "        \n",
    "    df = pd.DataFrame(cleaned_rows)\n",
    "    df.to_excel(f\"{name}_tick.xlsx\", index=False, header=header)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
